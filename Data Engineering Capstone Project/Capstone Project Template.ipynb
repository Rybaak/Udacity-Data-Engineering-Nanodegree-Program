{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# School Shootings US and Demographics data in a Data Warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Main project goal is to builds ETL pipe line that user 3 diffrent datasets and fit data in star-schema model. Then data will be used by data analytics to answer buissnes questions, or answer some historical questions.\n",
    "Task to do is about join 3 datasets which contain data about \"US Cities: Demographics\", \"School Shootings US 1990-present\" and \"FBI NICS Firearm Background Check Data\".\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "Project will merge 3 diffrent CSV data sets to a single dimensional model in data warehouse data sientists. I've chosen Apache Spark for exploration and cleaning data, and AWS S3 as storage service.\n",
    "Next for data warehouse purpose I choose AWS Redshift. In Redshift data will be staged and transformed to data model. \n",
    "Full data model can answer questions about School Shootings in correlation of US cities demographics and number of firearm checks by month and state.\n",
    "\n",
    "To do a little bit more complicated, after Apache Spark part, one set of data is stored in PARQUET format.\n",
    "\n",
    "##### Describe and Gather Data \n",
    "\n",
    "\n",
    "##### US Cities: Demographics\n",
    "\n",
    "us-cities-demographics.csv https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/ \n",
    "\n",
    "This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. \n",
    "\n",
    "This data comes from the US Census Bureau's 2015 American Community Survey.\n",
    "\n",
    "* City\n",
    "* State\n",
    "* Median Age\n",
    "* Male Population\n",
    "* Female Population\n",
    "* Total Population\n",
    "* Number of Veterans\n",
    "* Foreign-born\n",
    "* Average Household Size\n",
    "* State Code\n",
    "* Race\n",
    "* Count\n",
    "\n",
    "Ex:\n",
    "Row(City='Los Angeles', State='California', Median Age='35.0', Male Population='1958998', Female Population='2012898', Total Population='3971896', Number of Veterans='85417', Foreign-born='1485425', Average Household Size='2.86', State Code='CA', Race='White', Count='2177650')\n",
    "\n",
    "#####  School Shootings US 1990-present \n",
    "\n",
    "pah_wikp_combo.csv - https://www.kaggle.com/ecodan/school-shootings-us-1990present#pah_wikp_combo.csv\n",
    "\n",
    "A list of all school shooting incidents from 1990 to present.\n",
    "\n",
    "* Date: date of incident\n",
    "* City: location of incident\n",
    "* State: location of incident\n",
    "* Area Type: urban or suburban (only in Pah dataset)\n",
    "* School: C = college, HS = high school, MS = middle school, ES = elementary school, - = unknown\n",
    "* Fatalities: # killed\n",
    "* Wounded: # wounded (only in Wikipedia dataset)\n",
    "* Dupe: whether this incident appears in both datasets. Note: only the \"Pah\" version of the incident is marked.\n",
    "* Source: Pah or Wikp\n",
    "* Desc: text description of incident (only in Wikipedia dataset)\n",
    "\n",
    "Ex:\n",
    "Row(Date='3/27/90', City='Brooklyn', State='New York', AreaType=None, School='C', Fatalities='0', Wounded='1', Dupe=None, Source='Wikp', Desc='A black youth was taunted with racial slurs by three white youths in the stairwell of a public school in the¬†Bensonhurst¬†area of¬†Brooklyn. The 14-year-old was then shot and slightly wounded, because he had acted as peacemaker when the same boys had clashed with another black teen the month before.[240]')\n",
    "\n",
    "##### FBI NICS Firearm Background Check Data\n",
    "\n",
    "nics-firearm-background-checks.csv - https://github.com/BuzzFeedNews/nics-firearm-background-checks\n",
    "\n",
    "The data in this repository comes from the FBI's National Instant Criminal Background Check System. The FBI provides data on the number of firearm checks by month, state, and type.\n",
    "\n",
    "* month\n",
    "* state\n",
    "* permit\n",
    "* permit_recheck\n",
    "* handgun\n",
    "* long_gun\n",
    "* other\n",
    "* multiple\n",
    "* admin\n",
    "* prepawn_handgun\n",
    "* prepawn_long_gun\n",
    "* prepawn_other\n",
    "* redemption_handgun\n",
    "* redemption_long_gun\n",
    "* redemption_other\n",
    "* returned_handgun\n",
    "* returned_long_gun\n",
    "* returned_other\n",
    "* rentals_handgun\n",
    "* rentals_long_gun\n",
    "* private_sale_handgun\n",
    "* private_sale_long_gun\n",
    "* private_sale_other\n",
    "* return_to_seller_handgun\n",
    "* return_to_seller_long_gun\n",
    "* return_to_seller_other\n",
    "* totals\n",
    "\n",
    "Ex:\n",
    "Row(month='2020-03', state='Alabama', permit='31205', permit_recheck='606', handgun='34897', long_gun='17850', other='1583', multiple='1744', admin='0', prepawn_handgun='36', prepawn_long_gun='23', prepawn_other='0', redemption_handgun='3035', redemption_long_gun='1564', redemption_other='19', returned_handgun='13', returned_long_gun='0', returned_other='0', rentals_handgun='0', rentals_long_gun='0', private_sale_handgun='42', private_sale_long_gun='23', private_sale_other='8', return_to_seller_handgun='2', return_to_seller_long_gun='2', return_to_seller_other='0', totals='92652')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data & Cleaning Steps\n",
    "\n",
    "About exploration I need to see that events overlap each other in data sets.\n",
    "Data quality is at a good level. Before load to Redshift, format column with dates of fact should be made consistent.\n",
    "\n",
    "Data about \"US Cities: Demographics\" needs to be aggregated from \"Race\" to \"City\", and then from \"City\" to \"State\", to fit rest of data.\n",
    "This step is don in Redshift in part from staging table to dimensian table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['CONFIG_SPARK']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['CONFIG_SPARK']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "DEMOGRAPHICS_S3='s3a://udacity-capstone-input/us-cities-demographics.csv'\n",
    "SHOOTINGS_S3='s3a://udacity-capstone-input/pah_wikp_combo.csv'\n",
    "FIREARM_S3='s3a://udacity-capstone-input/nics-firearm-background-checks.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.option('header', 'true').option(\"delimiter\", \";\").csv(DEMOGRAPHICS_S3)\n",
    "df.createOrReplaceTempView(\"demographics_view\")\n",
    "\n",
    "df = spark.read.option('header', 'true').csv(FIREARM_S3)\n",
    "df.createOrReplaceTempView(\"firearm_view\")\n",
    "\n",
    "df = spark.read.option('header', 'true').csv(SHOOTINGS_S3)\n",
    "df.createOrReplaceTempView(\"shootings_view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_table = spark.sql(\"\"\"\n",
    "select \n",
    "*\n",
    "from demographics_view\n",
    "\"\"\")\n",
    "demographics_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "firearm_table = spark.sql(\"\"\"\n",
    "select \n",
    "year(month) as year_partition,\n",
    "month(month) as month_partition,\n",
    "*\n",
    "from firearm_view where month like \"2013-01\" \n",
    "\"\"\")\n",
    "firearm_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "shootings_table = spark.sql(\"\"\"\n",
    "select \n",
    "date_format(to_date(Date,\"MM/dd/yy\"),\"YYYY-MM\") as new_date,\n",
    "year(to_date(Date,\"MM/dd/yy\")) as year_partition,\n",
    "month(to_date(Date,\"MM/dd/yy\")) as month_partition,\n",
    "*\n",
    "from shootings_view where date_format(to_date(Date,\"MM/dd/yy\"),\"YYYY-MM\") = \"2013-01\"\n",
    "\"\"\")\n",
    "shootings_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "![image info](data_model.jpg)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "0. Fill credentials in dwh.cfg\n",
    "1. Load CSV from S3 bucket into Apache Spark (local mode)\n",
    "2. Make TempView, rename columns and cast date.\n",
    "3. Save to S3 output bucket - CSV and PARQUET\n",
    "4. Drop stage tables in Redshift \n",
    "5. Load data into stage and analyze table\n",
    "6. Transform by SQL insert into fact and dimensions\n",
    "7. Analyze target table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print('--- Config START ---')\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['CONFIG_SPARK']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['CONFIG_SPARK']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "DEMOGRAPHICS_S3='s3a://udacity-capstone-input/us-cities-demographics.csv'\n",
    "SHOOTINGS_S3='s3a://udacity-capstone-input/pah_wikp_combo.csv'\n",
    "FIREARM_S3='s3a://udacity-capstone-input/nics-firearm-background-checks.csv'\n",
    "\n",
    "output_data='s3a://udacity-capstone-output2/'\n",
    "\n",
    "print('--- Config DONE ---')\n",
    "\n",
    "print('--- Spark session & file & view  START ---')\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.option('header', 'true').option(\"delimiter\", \";\").csv(DEMOGRAPHICS_S3)\n",
    "df.createOrReplaceTempView(\"demographics_view\")\n",
    "\n",
    "df = spark.read.option('header', 'true').csv(FIREARM_S3)\n",
    "df.createOrReplaceTempView(\"firearm_view\")\n",
    "\n",
    "df = spark.read.option('header', 'true').csv(SHOOTINGS_S3)\n",
    "df.createOrReplaceTempView(\"shootings_view\")\n",
    "\n",
    "print('--- Spark session & file & view DONE ---')\n",
    "\n",
    "print('--- demographics_table START ---')\n",
    "\n",
    "demographics_table = spark.sql(\"\"\"\n",
    "select \n",
    "*\n",
    "from demographics_view\n",
    "\"\"\")\n",
    "demographics_rename_table=demographics_table \\\n",
    ".withColumnRenamed(\"Median Age\",\"Median_Age\") \\\n",
    ".withColumnRenamed(\"Male Population\",\"Male_Population\") \\\n",
    ".withColumnRenamed(\"Female Population\",\"Female_Population\") \\\n",
    ".withColumnRenamed(\"Total Population\",\"Total_Population\") \\\n",
    ".withColumnRenamed(\"Number of Veterans\",\"Number_of_Veterans\") \\\n",
    ".withColumnRenamed(\"Foreign-born\",\"Foreign_born\") \\\n",
    ".withColumnRenamed(\"Average Household Size\",\"Average_Household_Size\") \\\n",
    ".withColumnRenamed(\"State Code\",\"State_Code\") \n",
    "\n",
    "demographics_table=demographics_rename_table\n",
    "\n",
    "print('--- demographics_table DONE ---')\n",
    "\n",
    "print('--- firearm_table START ---')\n",
    "\n",
    "firearm_table = spark.sql(\"\"\"\n",
    "select \n",
    "year(month) as year_partition,\n",
    "month(month) as month_partition,\n",
    "*\n",
    "from firearm_view\n",
    "\"\"\")\n",
    "\n",
    "print('--- firearm_table DONE ---')\n",
    "\n",
    "print('--- shootings_table START ---')\n",
    "\n",
    "shootings_table = spark.sql(\"\"\"\n",
    "select \n",
    "date_format(to_date(Date,\"MM/dd/yy\"),\"YYYY-MM\") as new_date,\n",
    "year(to_date(Date,\"MM/dd/yy\")) as year_partition,\n",
    "month(to_date(Date,\"MM/dd/yy\")) as month_partition,\n",
    "*\n",
    "from shootings_view\n",
    "\"\"\")\n",
    "\n",
    "shootings_table = shootings_table.drop(\"Desc\")\n",
    "\n",
    "print('--- shootings_table DONE ---')\n",
    "\n",
    "print('--- demographics_table write START ---')\n",
    "demographics_table.write.mode('overwrite').csv(output_data + 'demographics_table/')\n",
    "print('--- demographics_table write DONE ---')\n",
    "\n",
    "print('--- firearm_table write START ---')\n",
    "firearm_table.write.mode('overwrite').csv(output_data + 'firearm_table/')\n",
    "print('--- firearm_table write DONE ---')\n",
    "\n",
    "print('--- shootings_table write START ---')\n",
    "shootings_table.write.mode('overwrite').parquet(output_data + 'shootings_table/')\n",
    "print('--- shootings_table write DONE ---')\n",
    "\n",
    "print(\" --- ETL SPARK END ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import psycopg2\n",
    "from sql_queries import drop_table_queries, create_table_queries, copy_table_queries, analyze_stage_queries, create_dim_facts_table_queries, insert_dim_facts_table_queries, analyze_dim_facts_queries, analyze_dim_facts_queries\n",
    "\n",
    "def drop_staging_tables(cur, conn):\n",
    "    for query in drop_table_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_table_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def copy_tables(cur, conn):\n",
    "    for query in copy_table_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "def analyze_stage(cur, conn):\n",
    "    for query in analyze_stage_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def create_dim_facts_table(cur, conn):\n",
    "    for query in create_dim_facts_table_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "def insert_dim_facts_table(cur, conn):\n",
    "    for query in insert_dim_facts_table_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "def analyze_dim(cur, conn):\n",
    "    for query in analyze_dim_facts_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "        \n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()\n",
    "\n",
    "drop_staging_tables(cur, conn)\n",
    "create_tables(cur, conn)\n",
    "copy_tables(cur, conn)\n",
    "analyze_stage(cur, conn)\n",
    "create_dim_facts_table(cur, conn)\n",
    "insert_dim_facts_table(cur, conn)\n",
    "analyze_dim(cur, conn)\n",
    "\n",
    "print(\" --- ETL REDSHIFT END ---\")\n",
    "\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sql_queries import data_count_quality_check, data_state_unknown_quality_check\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()\n",
    "\n",
    "print('1. First quality check, number of records in model must be greater than 0.')\n",
    "print('--- ---')\n",
    "df = pd.read_sql_query(data_count_quality_check, conn)\n",
    "print(df)\n",
    "print('')\n",
    "print('')\n",
    "print('2. Second quality check, for state that is not in dim_state dimension. Should return Empty DataFrame.')\n",
    "print('--- ---')\n",
    "df = pd.read_sql_query(data_state_unknown_quality_check, conn)\n",
    "print(df)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "1. fact_shootings - table contains facts of school shooting incidents from 1990 to present\n",
    "2. dim_time - dimension of time, cotains all data from shootings with extracted hour/day/week/month/year/weekday\n",
    "3. dim_state - dimension with all possible state from shootings/firearm/demographics with new ID's\n",
    "4. dim_firearm_statistic - dimension provides data on the number of firearm checks by month, state, and type\n",
    "5. dim_demographics - dimension contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000\n",
    "\n",
    "#### Example SQL with join all fact and dimensions, for ex. 2015:\n",
    "\n",
    "```\n",
    "select distinct dt.*,\n",
    "                ss.*,\n",
    "                dstate.*,\n",
    "                dfs.*,\n",
    "                dd.*\n",
    "from fact_shootings ss\n",
    "         left outer join dim_time dt\n",
    "                         on ss.date = dt.date\n",
    "         left outer join dim_state dstate\n",
    "                         on ss.id_state = dstate.id\n",
    "         left outer join dim_firearm_statistic dfs\n",
    "                         on ss.id_state = dfs.ID_STATE and to_date(ss.date, 'yyyy-mm') = to_date(dfs.date, 'yyyy-mm')\n",
    "         left outer join dim_demographics dd\n",
    "                         on dstate.id = dd.ID_STATE\n",
    "where to_char(ss.date, 'yyyy') = '2015'\n",
    ";\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "I chose Apache spark because it easy to use, elastic and scalable if I needed to (easy to spin up EMR cluster on AWS and change mode to cluster). Second tool is Redshift, I am familiar with databases and SQL so it was a natural choice, and its easy to provide data model further into BI world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Propose how often the data should be updated and why.\n",
    "\n",
    "Model does not have to be updated, fact table and dimensions can be populated with new CSV data. Starschema automatically splits into dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The data was increased by 100x.\n",
    "\n",
    "At Apache Spark part, I would choose to create EMR cluster on AWS and change mode to cluster.  Data was stored on S3, so it will very efficient. On Redshift part, just add new \"worker nodes\", or change to more powerfull. Table was created with DISTSTYLE and SORTKEY, so data will be rebalance on new nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "I will be setup Apache Airflow to schedule DAG's to do updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The database needed to be accessed by 100+ people.\n",
    "It's Redshift, so add nodes and it will be autoscaling and increase read performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
